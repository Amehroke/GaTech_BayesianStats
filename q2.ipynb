{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2f0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTENSOR_FLAGS\"] = \"linker=py,cxx=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebd28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bea0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after conversion:\n",
      "pregnant       int64\n",
      "glucose      float64\n",
      "diastolic    float64\n",
      "triceps      float64\n",
      "insulin      float64\n",
      "bmi          float64\n",
      "diabetes     float64\n",
      "age            int64\n",
      "test           int64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "pregnant       0\n",
      "glucose        5\n",
      "diastolic     35\n",
      "triceps      227\n",
      "insulin      374\n",
      "bmi           11\n",
      "diabetes       0\n",
      "age            0\n",
      "test           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Final/pima.csv')\n",
    "df = df.replace('NA', np.nan)\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "print(\"Data types after conversion:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e0b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized and Mean-Imputed Data Head:\n",
      "   pregnant   glucose  diastolic   triceps   insulin       bmi  diabetes  \\\n",
      "0  0.639947  0.862287  -0.032746  0.558557  0.000000  0.165097  0.468492   \n",
      "1 -0.844885 -1.202229  -0.517645 -0.014657  0.000000 -0.846404 -0.365061   \n",
      "2  1.233880  2.009241  -0.679278  0.000000  0.000000 -1.323254  0.604397   \n",
      "3 -0.844885 -1.071148  -0.517645 -0.587871 -0.518847 -0.629654 -0.920763   \n",
      "4 -1.141852  0.501816  -2.618874  0.558557  0.104968  1.537847  5.484909   \n",
      "\n",
      "        age  \n",
      "0  1.425995  \n",
      "1 -0.190672  \n",
      "2 -0.105584  \n",
      "3 -1.041549  \n",
      "4 -0.020496  \n",
      "\n",
      "Check for remaining NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "predictors = df.columns.drop('test')\n",
    "X = df[predictors]\n",
    "y = df['test']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=predictors)\n",
    "X_imputed_mean = X_scaled_df.fillna(0)\n",
    "print(\"\\nStandardized and Mean-Imputed Data Head:\")\n",
    "print(X_imputed_mean.head())\n",
    "print(\"\\nCheck for remaining NaNs:\", X_imputed_mean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c04c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_part1 = {\"predictor\": predictors}\n",
    "with pm.Model(coords=coords_part1) as model_part1:\n",
    "    X_data = pm.Data(\"X_data\", X_imputed_mean.values, mutable=False)\n",
    "    y_data = pm.Data(\"y_data\", y.values, mutable=False)\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=2.5, dims=\"predictor\")\n",
    "    mu = intercept + pt.dot(X_data, beta)\n",
    "    likelihood = pm.Bernoulli(\"likelihood\", logit_p=mu, observed=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799a8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [intercept, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86418ef7cc545e69edfe5890e46bb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m model_part1:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     idata_part1 = \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcores\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pymc_env2/lib/python3.12/site-packages/pymc/sampling/mcmc.py:957\u001b[39m, in \u001b[36msample\u001b[39m\u001b[34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, compile_kwargs, **kwargs)\u001b[39m\n\u001b[32m    955\u001b[39m         _log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSequential sampling (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchains\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chains in 1 job)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    956\u001b[39m         _print_step_hierarchy(step)\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mjoined_blas_limiter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    958\u001b[39m             _sample_many(**sample_args)\n\u001b[32m    960\u001b[39m t_sampling = time.time() - t_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pymc_env2/lib/python3.12/site-packages/threadpoolctl.py:593\u001b[39m, in \u001b[36m_ThreadpoolLimiter.__exit__\u001b[39m\u001b[34m(self, type, value, traceback)\u001b[39m\n\u001b[32m    592\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrestore_original_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pymc_env2/lib/python3.12/site-packages/threadpoolctl.py:607\u001b[39m, in \u001b[36m_ThreadpoolLimiter.restore_original_limits\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Set the limits back to their original values\"\"\"\u001b[39;00m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lib_controller, original_info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    605\u001b[39m     \u001b[38;5;28mself\u001b[39m._controller.lib_controllers, \u001b[38;5;28mself\u001b[39m._original_info\n\u001b[32m    606\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[43mlib_controller\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_num_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_threads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pymc_env2/lib/python3.12/site-packages/threadpoolctl.py:197\u001b[39m, in \u001b[36mOpenBLASController.set_num_threads\u001b[39m\u001b[34m(self, num_threads)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_num_threads\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_threads):\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     set_num_threads_func = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_symbol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenblas_set_num_threads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_num_threads_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m set_num_threads_func(num_threads)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pymc_env2/lib/python3.12/site-packages/threadpoolctl.py:158\u001b[39m, in \u001b[36mLibController._get_symbol\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_symbol\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m    157\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the symbol of the shared library accounding for the affixes\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdynlib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_symbol_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_symbol_suffix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with model_part1:\n",
    "    idata_part1 = pm.sample(2000, tune=1000, cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_part1 = az.summary(idata_part1, var_names=[\"intercept\", \"beta\"], hdi_prob=0.95)\n",
    "print(\"Posterior Summary (Part 1 - Mean Imputation):\")\n",
    "print(summary_part1)\n",
    "print(\"\\nInterpretation of Significance (HDI excludes 0):\")\n",
    "significant_vars_part1 = summary_part1[(summary_part1['hdi_2.5%'] > 0) | (summary_part1['hdi_97.5%'] < 0)]\n",
    "print(significant_vars_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6184d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_part2 = {\"predictor\": predictors, \"obs_id\": df.index}\n",
    "n_predictors = X_scaled_df.shape[1]\n",
    "with pm.Model(coords=coords_part2) as model_part2:\n",
    "    mu_imp = pm.Normal(\"mu_imp\", mu=0, sigma=1, dims=\"predictor\")\n",
    "    sigma_imp = pm.HalfNormal(\"sigma_imp\", sigma=1, dims=\"predictor\")\n",
    "    X_imputed = pm.Normal(\n",
    "        \"X_imputed\",\n",
    "        mu=mu_imp,\n",
    "        sigma=sigma_imp,\n",
    "        observed=X_scaled_df.values,\n",
    "        dims=(\"obs_id\", \"predictor\")\n",
    "    )\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=2.5, dims=\"predictor\")\n",
    "    mu = intercept + pt.dot(X_imputed, beta)\n",
    "    likelihood = pm.Bernoulli(\"likelihood\", logit_p=mu, observed=y.values, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa0cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [mu_imp, sigma_imp, X_imputed_unobserved, intercept, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108c56f97117473caba644a56d87a1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with model_part2:\n",
    "    idata_part2 = pm.sample(2000, tune=1000, cores=1, target_accept=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_part2 = az.summary(idata_part2, var_names=[\"intercept\", \"beta\"], hdi_prob=0.95)\n",
    "print(\"Posterior Summary (Part 2 - Bayesian Imputation):\")\n",
    "print(summary_part2)\n",
    "print(\"\\nInterpretation of Significance (HDI excludes 0):\")\n",
    "significant_vars_part2 = summary_part2[(summary_part2['hdi_2.5%'] > 0) | (summary_part2['hdi_97.5%'] < 0)]\n",
    "print(significant_vars_part2)\n",
    "comparison = pd.DataFrame({\n",
    "    'Mean_Part1': summary_part1['mean'],\n",
    "    'Mean_Part2': summary_part2['mean'],\n",
    "    'SD_Part1': summary_part1['sd'],\n",
    "    'SD_Part2': summary_part2['sd']\n",
    "})\n",
    "print(\"\\nComparison of Coefficients (Mean Imputation vs Bayesian Imputation):\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dc632",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_part3 = {\"predictor\": predictors, \"obs_id\": df.index}\n",
    "n_predictors = X_scaled_df.shape[1]\n",
    "tau_0 = 0.01\n",
    "tau_1 = 2.5\n",
    "with pm.Model(coords=coords_part3) as model_part3:\n",
    "    mu_imp = pm.Normal(\"mu_imp\", mu=0, sigma=1, dims=\"predictor\")\n",
    "    sigma_imp = pm.HalfNormal(\"sigma_imp\", sigma=1, dims=\"predictor\")\n",
    "    X_imputed = pm.Normal(\n",
    "        \"X_imputed\",\n",
    "        mu=mu_imp,\n",
    "        sigma=sigma_imp,\n",
    "        observed=X_scaled_df.values,\n",
    "        dims=(\"obs_id\", \"predictor\")\n",
    "    )\n",
    "    delta = pm.Bernoulli(\"delta\", p=0.5, dims=\"predictor\")\n",
    "    sigma_beta = pm.math.switch(pt.eq(delta, 1), tau_1, tau_0)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=sigma_beta, dims=\"predictor\")\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n",
    "    mu = intercept + pt.dot(X_imputed, beta)\n",
    "    likelihood = pm.Bernoulli(\"likelihood\", logit_p=mu, observed=y.values, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ff760",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_part3:\n",
    "    idata_part3 = pm.sample(2000, tune=1000, cores=1, target_accept=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = idata_part3.posterior[\"delta\"].mean(dim=(\"chain\", \"draw\"))\n",
    "pip_df = pip.to_dataframe().rename(columns={'delta': 'Inclusion Probability'})\n",
    "print(\"Posterior Inclusion Probabilities (PIP):\")\n",
    "print(pip_df.sort_values(by='Inclusion Probability', ascending=False))\n",
    "delta_samples = idata_part3.posterior[\"delta\"].stack(sample=(\"chain\", \"draw\")).values.T\n",
    "model_configs = [tuple(row) for row in delta_samples]\n",
    "from collections import Counter\n",
    "model_counts = Counter(model_configs)\n",
    "total_samples = delta_samples.shape[0]\n",
    "model_probs = {config: count / total_samples for config, count in model_counts.items()}\n",
    "sorted_models = sorted(model_probs.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"\\nTop 5 Models by Posterior Probability:\")\n",
    "for i, (config, prob) in enumerate(sorted_models[:5]):\n",
    "    included_vars = [predictors[j] for j, included in enumerate(config) if included == 1]\n",
    "    print(f\"Model {i+1} (Prob: {prob:.4f}):\")\n",
    "    print(f\"  Included: {', '.join(included_vars) if included_vars else 'None (Intercept Only)'}\")\n",
    "    print(f\"  Config: {config}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
