{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Introduction to Diagnostic Testing Metrics/ meterics for Model Performance:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many students and professionals misunderstand test precision metrics (e.g., confusing test error rates with accuracy).\n",
    "- Example: Casscells et al. (1978) highlighted that most participants incorrectly interpreted the PPV of a diagnostic test with 1/1000 disease prevalence and 5% false positive rate. Correct answer: ~2%; common incorrect response: 95%.\n",
    "\n",
    "- If a model says a person has a disease, think of it as the prediction says the person has the disease, not that the person has the disease. It sounds weird, but what it is saying think of the prediction as a metric not Truth (Ground Truth). \n",
    "\n",
    "- The prediction reflects a probability or classification that should be interpreted alongside the model's sensitivity, specificity, and predictive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions for Sensitivity, Specificity, and Related Metrics\n",
    "\n",
    "1. **True Positive (TP):**  \n",
    "   A result where the test predicts a positive outcome, and the subject actually has the condition.\n",
    "\n",
    "2. **False Positive (FP):**  \n",
    "   A result where the test predicts a positive outcome, but the subject does not have the condition.\n",
    "\n",
    "3. **True Negative (TN):**  \n",
    "   A result where the test predicts a negative outcome, and the subject does not have the condition.\n",
    "\n",
    "4. **False Negative (FN):**  \n",
    "   A result where the test predicts a negative outcome, but the subject actually has the condition.\n",
    "\n",
    "5. **Sensitivity (Recall, True Positive Rate):**  \n",
    "   $$\n",
    "   \\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "   $$  \n",
    "   Measures the ability of a test to correctly identify positive cases.\n",
    "\n",
    "6. **Specificity (True Negative Rate):**  \n",
    "   $$\n",
    "   \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "   $$  \n",
    "   Measures the ability of a test to correctly identify negative cases.\n",
    "\n",
    "7. **Positive Predictive Value (PPV):**  \n",
    "   $$\n",
    "   \\text{PPV} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "   $$  \n",
    "   Probability that a subject with a positive test result actually has the condition.\n",
    "\n",
    "8. **Negative Predictive Value (NPV):**  \n",
    "   $$\n",
    "   \\text{NPV} = \\frac{\\text{TN}}{\\text{TN} + \\text{FN}}\n",
    "   $$  \n",
    "   Probability that a subject with a negative test result does not have the condition.\n",
    "\n",
    "9. **Accuracy:**  \n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n",
    "   $$  \n",
    "   Measures the overall correctness of the test across all cases.\n",
    "\n",
    "10. **Prevalence:**  \n",
    "    $$\n",
    "    \\text{Prevalence} = \\frac{\\text{TP} + \\text{FN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n",
    "    $$  \n",
    "    The proportion of the population with the condition.\n",
    "\n",
    "11. **False Positive Rate (FPR):**  \n",
    "    $$\n",
    "    \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "    $$  \n",
    "    The probability of falsely identifying a negative case as positive.\n",
    "\n",
    "12. **False Negative Rate (FNR):**  \n",
    "    $$\n",
    "    \\text{FNR} = \\frac{\\text{FN}}{\\text{TP} + \\text{FN}}\n",
    "    $$  \n",
    "    The probability of failing to identify a positive case.\n",
    "\n",
    "13. **Receiver Operating Characteristic (ROC) Curve:**  \n",
    "    A plot of **Sensitivity** ($y$-axis) vs. \\(1 - \\text{Specificity}\\) ($x$-axis), used to evaluate the performance of a diagnostic test.\n",
    "\n",
    "14. **Area Under the Curve (AUC):**  \n",
    "    The area under the ROC curve, representing the test's ability to discriminate between positive and negative cases.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Table Representation\n",
    "\n",
    "| Disease (D)        | No Disease (C)   | Total                |\n",
    "|---------------------|------------------|----------------------|\n",
    "| **Test Positive (P)** | TP              | FP                   | $$n_P = TP + FP$$ |\n",
    "| **Test Negative (N)** | FN              | TN                   | $$n_N = FN + TN$$ |\n",
    "| **Total**           | $$n_D = TP + FN$$| $$n_C = FP + TN$$    | $$n = n_D + n_C$$ |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
