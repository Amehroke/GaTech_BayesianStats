{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Introduction to Diagnostic Testing Metrics/ meterics for Model Performance:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many students and professionals misunderstand test precision metrics (e.g., confusing test error rates with accuracy).\n",
    "- Example: Casscells et al. (1978) highlighted that most participants incorrectly interpreted the PPV of a diagnostic test with 1/1000 disease prevalence and 5% false positive rate. Correct answer: ~2%; common incorrect response: 95%.\n",
    "\n",
    "- If a model says a person has a disease, think of it as the prediction says the person has the disease, not that the person has the disease. It sounds weird, but what it is saying think of the prediction as a metric not Truth (Ground Truth). \n",
    "\n",
    "- The prediction reflects a probability or classification that should be interpreted alongside the model's sensitivity, specificity, and predictive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions for Sensitivity, Specificity, and Related Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **True Positive (TP):**  \n",
    "   A result where the test predicts a positive outcome, and the subject actually has the condition.\n",
    "\n",
    "2. **False Positive (FP):**  \n",
    "   A result where the test predicts a positive outcome, but the subject does not have the condition.\n",
    "\n",
    "3. **True Negative (TN):**  \n",
    "   A result where the test predicts a negative outcome, and the subject does not have the condition.\n",
    "\n",
    "4. **False Negative (FN):**  \n",
    "   A result where the test predicts a negative outcome, but the subject actually has the condition.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Sensitivity (Recall, True Positive Rate):**  \n",
    "   $$\n",
    "   \\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "   $$  \n",
    "- Measures how well the model identifies true positives.\n",
    "- Useful when false negatives are critical (e.g., detecting diseases, fraud).\n",
    "---\n",
    "6. **Specificity (True Negative Rate):**  \n",
    "   $$\n",
    "   \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "   $$  \n",
    "- Measures the ability of a test to correctly identify negative cases.\n",
    "- Measures how well the model identifies true negatives.\n",
    "- Important when false positives are costly (e.g., unnecessary treatments or alarms)\n",
    "---\n",
    "7. **Precision (Positive Predictive Value (PPV))-book def:**  \n",
    "   $$\n",
    "   \\text{PPV} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "   $$  \n",
    "- Probability that a subject with a positive test result actually has the condition.\n",
    "---\n",
    "8. **Negative Predictive Value (NPV):**  \n",
    "   $$\n",
    "   \\text{NPV} = \\frac{\\text{TN}}{\\text{TN} + \\text{FN}}\n",
    "   $$  \n",
    "- Probability that a subject with a negative test result does not have the condition.\n",
    "- Rarely referred to directly in ML\n",
    "---\n",
    "9. **Accuracy:**  \n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n",
    "   $$  \n",
    "- Measures the overall correctness of the test across all cases.\n",
    "- Most common metric but can be misleading for imbalanced datasets.\n",
    "\n",
    "---\n",
    "10. **F1-Score:**  \n",
    "    $$\n",
    "    \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "    $$  \n",
    "- The harmonic mean of precision and recall, balancing false positives and false negatives.\n",
    "- Balances precision and recall, commonly used in imbalanced datasets.\n",
    "\n",
    "---\n",
    "11. **Prevalence:**  \n",
    "    $$\n",
    "    \\text{Prevalence} = \\frac{\\text{TP} + \\text{FN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n",
    "    $$  \n",
    "- The proportion of the population with the condition.\n",
    "---\n",
    "12. **False Positive Rate (FPR):**  \n",
    "    $$\n",
    "    \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "    $$  \n",
    "- The probability of falsely identifying a negative case as positive.\n",
    "- Equivalent to (1 - Specificity) a.k.a \"Fallout.\"\n",
    "\n",
    "---\n",
    "13. **False Negative Rate (FNR):**  \n",
    "    $$\n",
    "    \\text{FNR} = \\frac{\\text{FN}}{\\text{TP} + \\text{FN}}\n",
    "    $$  \n",
    "- The probability of failing to identify a positive case.\n",
    "- Equivalent to (1 - Sensitivity)\n",
    "\n",
    "---\n",
    "14. **Receiver Operating Characteristic (ROC) Curve:**  \n",
    "    A plot of **Sensitivity** ($y$-axis) vs. \\(1 - Specificity}\\) ($x$-axis), used to evaluate the performance of a diagnostic test.\n",
    "- Visual tool to compare classification models.\n",
    "---\n",
    "15. **Area Under the Curve (AUC):**  \n",
    "    The area under the ROC curve, representing the test's ability to discriminate between positive and negative cases.\n",
    "- Metric for overall model performance, with values ranging from 0.5 (random guess) to 1 (perfect model).\n",
    "---\n",
    "\n",
    "#### Example Table Representation\n",
    "\n",
    "| Disease (D)        | No Disease (C)   | Total                |\n",
    "|---------------------|------------------|----------------------|\n",
    "| **Test Positive (P)** | TP              | FP                   | $$n_P = TP + FP$$ |\n",
    "| **Test Negative (N)** | FN              | TN                   | $$n_N = FN + TN$$ |\n",
    "| **Total**           | $$n_D = TP + FN$$| $$n_C = FP + TN$$    | $$n = n_D + n_C$$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Examples of Each Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification Problem Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
