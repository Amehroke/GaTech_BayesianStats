{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c844af",
   "metadata": {},
   "source": [
    "# Final Exam Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f237dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTENSOR_FLAGS\"] = \"linker=py,cxx=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79688e",
   "metadata": {},
   "source": [
    "### 1. Fit a multinomial logistic regression to the data with the same prior from the lecture and provide the posterior summary output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ffeec",
   "metadata": {},
   "source": [
    "### 2. Find the probability that a customer is satisfied or very satisfied when the delay is 1 minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d63554",
   "metadata": {},
   "source": [
    "### 3. Give the odds ratio (and credible set) that a passenger would report ”Satisfied” vs. ”Very Dissatisfied” per minute of delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ae8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# 1. Load and clean data\n",
    "df = pd.read_csv('bus.csv')\n",
    "\n",
    "# Map the actual strings in bus.csv to 0–4\n",
    "mapping = {\n",
    "    'V.dis':   0,   # very dissatisfied\n",
    "    'Dis':     1,   # dissatisfied\n",
    "    'Neutral': 2,   # neutral\n",
    "    'Satis':   3,   # satisfied\n",
    "    'V.satis': 4    # very satisfied\n",
    "}\n",
    "df['y'] = df['satisfaction'].map(mapping)\n",
    "\n",
    "# Drop any rows that failed to map\n",
    "df = df.dropna(subset=['y'])\n",
    "df['y'] = df['y'].astype(int)\n",
    "\n",
    "# 2. Prepare predictor\n",
    "X_raw = df['delay'].astype(float).values    # delays in {0,2,5,7}\n",
    "Xc = (X_raw - X_raw.mean()) / X_raw.std()   # center & scale\n",
    "X = Xc[:, None]                             # shape (n_samples, 1)\n",
    "\n",
    "# 3. Build and sample the multinomial logistic model\n",
    "with pm.Model() as model1:\n",
    "    # Priors from lecture: Normal(0,10)\n",
    "    α = pm.Normal('α', mu=0, sigma=10, shape=4)\n",
    "    β = pm.Normal('β', mu=0, sigma=10, shape=4)\n",
    "\n",
    "    # Logits: cat 0 has logit=0; cats 1–4 get α + β·X\n",
    "    logits = pm.math.concatenate([\n",
    "        pm.math.zeros((X.shape[0], 1)),\n",
    "        α + β * X\n",
    "    ], axis=1)\n",
    "\n",
    "    # Likelihood\n",
    "    obs = pm.Categorical(\n",
    "        'obs',\n",
    "        logit_p=logits,\n",
    "        observed=df['y'].values\n",
    "    )\n",
    "\n",
    "    # Sampling\n",
    "    trace1 = pm.sample(\n",
    "        draws=4000,\n",
    "        tune=4000,\n",
    "        target_accept=0.99,\n",
    "        init='adapt_diag',\n",
    "        return_inferencedata=True\n",
    "    )\n",
    "\n",
    "# 4. Problem 1.1: Posterior summary for α and β\n",
    "summary = az.summary(\n",
    "    trace1,\n",
    "    var_names=['α','β'],\n",
    "    round_to=3\n",
    ")[['mean','sd','hdi_3%','hdi_97%','ess_bulk','r_hat']]\n",
    "print(\"1.1) Posterior summary for α and β:\\n\", summary)\n",
    "\n",
    "# 5. Problem 1.2: P(satisfied or very satisfied | delay = 1)\n",
    "α_samps = trace1.posterior['α'].stack(draws=('chain','draw')).values  # (4, N)\n",
    "β_samps = trace1.posterior['β'].stack(draws=('chain','draw')).values  # (4, N)\n",
    "n_draws = α_samps.shape[1]\n",
    "\n",
    "def prob_each_category(delay_min):\n",
    "    d0 = (delay_min - X_raw.mean()) / X_raw.std()\n",
    "    z0 = np.zeros(n_draws)\n",
    "    z_rest = α_samps + β_samps * d0\n",
    "    z = np.vstack([z0, z_rest])                      # (5, n_draws)\n",
    "    exp_z = np.exp(z - z.max(axis=0))\n",
    "    return exp_z / exp_z.sum(axis=0)\n",
    "\n",
    "p1 = prob_each_category(1)                           # (5, n_draws)\n",
    "p_sat_or_vsat = p1[3] + p1[4]\n",
    "mean_p = p_sat_or_vsat.mean()\n",
    "hdi_low, hdi_high = az.hdi(p_sat_or_vsat, hdi_prob=0.95)\n",
    "print(f\"\\n1.2) P(satisfied or very satisfied | delay=1) = {mean_p:.3f}\")\n",
    "print(f\"     95% CI = [{hdi_low:.3f}, {hdi_high:.3f}]\")\n",
    "\n",
    "# 6. Problem 1.3: Odds‐ratio for “Satisfied” vs “Very Dissatisfied”\n",
    "# slope for category 3 (“satisfied”) is β_samps[2]\n",
    "log_or = β_samps[2]\n",
    "or_mean = np.exp(log_or).mean()\n",
    "or_low, or_high = np.exp(az.hdi(log_or, hdi_prob=0.95))\n",
    "print(f\"\\n1.3) OR(satisfied vs very dissatisfied) per minute = {or_mean:.2f}\")\n",
    "print(f\"     95% CI = [{or_low:.2f}, {or_high:.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f972ea4",
   "metadata": {},
   "source": [
    "1.1 The posterior summaries for α and β coefficients show that all r̂ values are approximately 1.000 and effective sample sizes are large (>5000), indicating good convergence. The mean estimates are close to zero with relatively wide 95% highest density intervals (HDIs), reflecting moderate uncertainty in the delay effect on satisfaction categories.\n",
    "\n",
    "1.2\n",
    "The estimated probability that a customer is \"Satisfied\" or \"Very Satisfied\" when the delay is 1 minute is 0.401, with a 95% credible interval of [0.135, 0.678].\n",
    "\n",
    "1.3 \n",
    "The estimated odds ratio that a passenger reports \"Satisfied\" versus \"Very Dissatisfied\" per additional minute of delay is 1.39, with a 95% credible interval of [0.22, 5.04], suggesting a slight positive but uncertain association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222f772",
   "metadata": {},
   "source": [
    "# Final Exam Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pytensor.tensor as pt\n",
    "import numpy.ma as ma\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# === Data Loading ===\n",
    "df = pd.read_csv('pima.csv')\n",
    "X = df.drop(columns=['test'])\n",
    "y = df['test']\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Save mean-imputed version for Part 1 and SSVS\n",
    "X_mean_imputed = X_scaled.fillna(X_scaled.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_part1:\n",
    "    intercept = pm.Normal('intercept', 0, 1)\n",
    "    coefs = pm.Normal('coefs', 0, 1, shape=X_mean_imputed.shape[1])\n",
    "\n",
    "    logits = intercept + pm.math.dot(X_mean_imputed.values, coefs)\n",
    "    \n",
    "    pm.Bernoulli('outcome', logit_p=logits, observed=y.values)\n",
    "    \n",
    "    trace_part1 = pm.sample(2000, tune=1000, target_accept=0.9, cores=1, random_seed=11)\n",
    "\n",
    "summary_part1 = az.summary(trace_part1, hdi_prob=0.95)\n",
    "print(\"Summary for 2.1 (Mean Imputation):\\n\", summary_part1)\n",
    "\n",
    "significant_2_1 = summary_part1[(summary_part1['hdi_2.5%'] > 0) | (summary_part1['hdi_97.5%'] < 0)]\n",
    "print(\"\\nSignificant predictors (2.1):\\n\", significant_2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72524c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pytensor.tensor as pt\n",
    "import numpy.ma as ma\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# === Data Loading ===\n",
    "df = pd.read_csv('pima.csv')\n",
    "X = df.drop(columns=['test'])\n",
    "y = df['test']\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Save mean-imputed version for Part 1 and SSVS\n",
    "X_mean_imputed = X_scaled.fillna(X_scaled.mean())\n",
    "\n",
    "# === 2.2 - Logistic Regression with Bayesian Imputation (Cleaned) ===\n",
    "predictors = X_scaled.columns\n",
    "\n",
    "# Set up coordinates\n",
    "coords_part2 = {\"predictor\": predictors, \"obs_id\": df.index}\n",
    "n_predictors = X_scaled.shape[1]  # fixed here\n",
    "\n",
    "with pm.Model(coords=coords_part2) as model_part2:\n",
    "    # Priors for missing data imputation\n",
    "    mu_imp = pm.Normal(\"mu_imp\", mu=0, sigma=1, dims=\"predictor\")\n",
    "    sigma_imp = pm.HalfNormal(\"sigma_imp\", sigma=1, dims=\"predictor\")\n",
    "    \n",
    "    # Imputed predictors (treating missing values probabilistically)\n",
    "    X_imputed = pm.Normal(\n",
    "        \"X_imputed\",\n",
    "        mu=mu_imp,\n",
    "        sigma=sigma_imp,\n",
    "        observed=X_scaled.values,  # fixed here\n",
    "        dims=(\"obs_id\", \"predictor\")\n",
    "    )\n",
    "\n",
    "    # Logistic regression part\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=2.5, dims=\"predictor\")\n",
    "\n",
    "    mu = intercept + pt.dot(X_imputed, beta)\n",
    "\n",
    "    # Likelihood\n",
    "    likelihood = pm.Bernoulli(\"likelihood\", logit_p=mu, observed=y.values, dims=\"obs_id\")\n",
    "    \n",
    "    # Faster and robust sampling\n",
    "    trace_part2 = pm.sample(\n",
    "        draws=10, \n",
    "        tune=500, \n",
    "        chains=4, \n",
    "        cores=4, \n",
    "        target_accept=0.95, \n",
    "        random_seed=22\n",
    "    )\n",
    "\n",
    "# === Summarize results for 2.2 ===\n",
    "summary_part2 = az.summary(trace_part2, var_names=[\"intercept\", \"beta\"], hdi_prob=0.95)\n",
    "print(\"\\nPosterior Summary (2.2 - Bayesian Imputation):\")\n",
    "print(summary_part2)\n",
    "\n",
    "# Identify significant predictors\n",
    "significant_vars_part2 = summary_part2[\n",
    "    (summary_part2[\"hdi_2.5%\"] > 0) | (summary_part2[\"hdi_97.5%\"] < 0)\n",
    "]\n",
    "print(\"\\nSignificant Variables (2.2 - HDI excludes 0):\")\n",
    "print(significant_vars_part2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
